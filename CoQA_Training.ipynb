{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoQA_Training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBfvQz45eOixxHxeiVuD4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Conv-AI/coqa-bert-baselines/blob/master/CoQA_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TVuyIrS_Wol",
        "outputId": "56074de6-5a19-410e-a1ad-dd0273efe293"
      },
      "source": [
        "!git clone https://github.com/Conv-AI/coqa-bert-baselines.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'coqa-bert-baselines'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 539 (delta 67), reused 86 (delta 33), pack-reused 413\u001b[K\n",
            "Receiving objects: 100% (539/539), 266.43 KiB | 17.76 MiB/s, done.\n",
            "Resolving deltas: 100% (312/312), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnMH7UY-Azm1",
        "outputId": "58755898-f6ec-47bb-bf89-47de33c00bff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd coqa-bert-baselines/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/coqa-bert-baselines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs_G2n1aBpSW"
      },
      "source": [
        "!cp /content/drive/MyDrive/CoQA_data_preprocessed/coqa.train.json /content/coqa-bert-baselines/\n",
        "!cp /content/drive/MyDrive/CoQA_data_preprocessed/coqa.dev.json /content/coqa-bert-baselines/ "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJJix93aCDX8",
        "outputId": "05382925-04dc-401f-87f5-6c89f8bf9a75"
      },
      "source": [
        "!pip install torch transformers textacy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 12.1MB/s \n",
            "\u001b[?25hCollecting textacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/99/054efc5dea92c84a850639c490541de6cba29bc148debc3c73848c5e64c2/textacy-0.10.1-py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.17.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.5)\n",
            "Requirement already satisfied: scikit-learn<0.24.0,>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.22.2.post1)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.2.4)\n",
            "Requirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.1.1)\n",
            "Collecting cytoolz>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/67/1c60da8ba831bfefedb64c78b9f6820bdf58972797c95644ee3191daf27a/cytoolz-0.11.0.tar.gz (477kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 49.7MB/s \n",
            "\u001b[?25hCollecting jellyfish>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/09/927ae35fc5a9f70abb6cc2c27ee88fc48549f7bc4786c1d4b177c22e997d/jellyfish-0.8.2-cp36-cp36m-manylinux2014_x86_64.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.0.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.4.1)\n",
            "Collecting pyphen>=0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->textacy) (4.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (2.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (50.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.1.3)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.11.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses, cytoolz\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a033a55d4d5d79f140f0aaeca3ee6b5a87ab8cf967e752c5ca66e5d3f68c9785\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.11.0-cp36-cp36m-linux_x86_64.whl size=1225594 sha256=909ca52340e7534eba9ffe86f17c1cfdf92a33de5560d52e006baa98d61446cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/32/3c/9c9926b510647cacdde744b2c7acdf1ccd5896fbb7f8d5df0c\n",
            "Successfully built sacremoses cytoolz\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, cytoolz, jellyfish, pyphen, textacy\n",
            "Successfully installed cytoolz-0.11.0 jellyfish-0.8.2 pyphen-0.10.0 sacremoses-0.0.43 textacy-0.10.1 tokenizers-0.9.4 transformers-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmvCpUINvAWa"
      },
      "source": [
        "%mkdir output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dv8QOviB0T6",
        "outputId": "bd0113b8-af8d-4929-babe-7f32e1f0cce9"
      },
      "source": [
        "!python main.py --trainset=\"./coqa.train.json\" --devset=\"./coqa.dev.json\" --model_name=\"BERT\" --save_state_dir=\"./output\" --n_history=2 --batch-size=2 --lr=5e-5 --gradient_accumulation_steps=10"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-04 07:53:58.923492: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 20.5MB/s]\n",
            "100% 7199/7199 [2:56:19<00:00,  1.47s/it]\n",
            "Load 7199 paragraphs, 108647 examples.\n",
            "Paragraph length: avg = 321.4, max = 1208\n",
            "Question length: avg = 98.2, max = 641\n",
            "Length of examples:  108647\n",
            "900it [01:06, 13.46it/s]\n",
            "100% 500/500 [11:25<00:00,  1.37s/it]\n",
            "Load 500 paragraphs, 7983 examples.\n",
            "Paragraph length: avg = 313.8, max = 1003\n",
            "Question length: avg = 97.6, max = 311\n",
            "Length of examples:  7983\n",
            "900it [01:28, 10.14it/s]\n",
            "Downloading: 100% 433/433 [00:00<00:00, 517kB/s]\n",
            "Downloading: 100% 440M/440M [00:04<00:00, 92.5MB/s]\n",
            "dir doesn't exists, cannot restore\n",
            "\n",
            ">>> Dev Epoch: [0 / 20]\n",
            "[predict-0] step: [200 / 531] | f1 = 3.33 | em = 0.00\n",
            "used_time: 14.38s\n",
            "[predict-0] step: [400 / 531] | f1 = 2.35 | em = 0.00\n",
            "used_time: 29.04s\n",
            "Validation Epoch 0 -- F1: 2.46, EM: 0.00 --\n",
            "\n",
            ">>> Train Epoch: [1 / 20]\n",
            "[train-1] step: [200 / 542] | exs = 400 | loss = 0.5577 | f1 = 0.00 | em = 0.00\n",
            "used_time: 54.47s\n",
            "[train-1] step: [400 / 542] | exs = 800 | loss = 0.2877 | f1 = 50.00 | em = 50.00\n",
            "used_time: 112.52s\n",
            "Training Epoch 1 -- Loss: 0.4852, F1: 20.12, EM: 19.72 --\n",
            "\n",
            ">>> Dev Epoch: [1 / 20]\n",
            "[predict-1] step: [200 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 17.07s\n",
            "[predict-1] step: [400 / 531] | f1 = 2.14 | em = 0.00\n",
            "used_time: 33.95s\n",
            "Validation Epoch 1 -- F1: 5.52, EM: 3.39 --\n",
            "!!! Updated: F1: 5.52, EM: 3.39\n",
            "\n",
            ">>> Train Epoch: [2 / 20]\n",
            "[train-2] step: [200 / 542] | exs = 1485 | loss = 0.5927 | f1 = 0.00 | em = 0.00\n",
            "used_time: 58.99s\n",
            "[train-2] step: [400 / 542] | exs = 1885 | loss = 0.6454 | f1 = 0.00 | em = 0.00\n",
            "used_time: 117.20s\n",
            "Training Epoch 2 -- Loss: 0.4285, F1: 23.39, EM: 21.94 --\n",
            "\n",
            ">>> Dev Epoch: [2 / 20]\n",
            "[predict-2] step: [200 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 17.05s\n",
            "[predict-2] step: [400 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 34.07s\n",
            "Validation Epoch 2 -- F1: 21.52, EM: 20.53 --\n",
            "!!! Updated: F1: 21.52, EM: 20.53\n",
            "\n",
            ">>> Train Epoch: [3 / 20]\n",
            "[train-3] step: [200 / 542] | exs = 2570 | loss = 0.3870 | f1 = 0.00 | em = 0.00\n",
            "used_time: 58.75s\n",
            "[train-3] step: [400 / 542] | exs = 2970 | loss = 0.4377 | f1 = 50.00 | em = 50.00\n",
            "used_time: 116.69s\n",
            "Training Epoch 3 -- Loss: 0.3683, F1: 29.58, EM: 28.20 --\n",
            "\n",
            ">>> Dev Epoch: [3 / 20]\n",
            "[predict-3] step: [200 / 531] | f1 = 50.00 | em = 50.00\n",
            "used_time: 17.05s\n",
            "[predict-3] step: [400 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 33.83s\n",
            "Validation Epoch 3 -- F1: 21.34, EM: 20.06 --\n",
            "\n",
            ">>> Train Epoch: [4 / 20]\n",
            "[train-4] step: [200 / 542] | exs = 3655 | loss = 0.2952 | f1 = 50.00 | em = 50.00\n",
            "used_time: 58.28s\n",
            "[train-4] step: [400 / 542] | exs = 4055 | loss = 0.1891 | f1 = 50.00 | em = 50.00\n",
            "used_time: 116.55s\n",
            "Training Epoch 4 -- Loss: 0.2852, F1: 33.90, EM: 31.15 --\n",
            "\n",
            ">>> Dev Epoch: [4 / 20]\n",
            "[predict-4] step: [200 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 17.08s\n",
            "[predict-4] step: [400 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 33.87s\n",
            "Validation Epoch 4 -- F1: 20.49, EM: 18.93 --\n",
            "\n",
            ">>> Train Epoch: [5 / 20]\n",
            "[train-5] step: [200 / 542] | exs = 4740 | loss = 0.0783 | f1 = 51.59 | em = 50.00\n",
            "used_time: 58.53s\n",
            "[train-5] step: [400 / 542] | exs = 5140 | loss = 0.1711 | f1 = 50.00 | em = 50.00\n",
            "used_time: 116.75s\n",
            "Training Epoch 5 -- Loss: 0.2018, F1: 46.82, EM: 43.69 --\n",
            "\n",
            ">>> Dev Epoch: [5 / 20]\n",
            "[predict-5] step: [200 / 531] | f1 = 6.96 | em = 0.00\n",
            "used_time: 17.09s\n",
            "[predict-5] step: [400 / 531] | f1 = 50.00 | em = 50.00\n",
            "used_time: 34.06s\n",
            "Validation Epoch 5 -- F1: 19.55, EM: 18.46 --\n",
            "\n",
            ">>> Train Epoch: [6 / 20]\n",
            "[train-6] step: [200 / 542] | exs = 5825 | loss = 0.0608 | f1 = 50.00 | em = 50.00\n",
            "used_time: 58.55s\n",
            "[train-6] step: [400 / 542] | exs = 6225 | loss = 0.2205 | f1 = 5.15 | em = 0.00\n",
            "used_time: 117.08s\n",
            "Training Epoch 6 -- Loss: 0.1350, F1: 62.57, EM: 60.55 --\n",
            "\n",
            ">>> Dev Epoch: [6 / 20]\n",
            "[predict-6] step: [200 / 531] | f1 = 50.00 | em = 50.00\n",
            "used_time: 17.09s\n",
            "[predict-6] step: [400 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 34.11s\n",
            "Validation Epoch 6 -- F1: 19.56, EM: 18.64 --\n",
            "\n",
            ">>> Train Epoch: [7 / 20]\n",
            "[train-7] step: [200 / 542] | exs = 6910 | loss = 0.0734 | f1 = 52.40 | em = 50.00\n",
            "used_time: 58.45s\n",
            "[train-7] step: [400 / 542] | exs = 7310 | loss = 0.1413 | f1 = 70.00 | em = 50.00\n",
            "used_time: 116.91s\n",
            "Training Epoch 7 -- Loss: 0.0895, F1: 72.83, EM: 70.32 --\n",
            "\n",
            ">>> Dev Epoch: [7 / 20]\n",
            "[predict-7] step: [200 / 531] | f1 = 4.07 | em = 0.00\n",
            "used_time: 17.09s\n",
            "[predict-7] step: [400 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 34.03s\n",
            "Validation Epoch 7 -- F1: 17.55, EM: 16.29 --\n",
            "\n",
            ">>> Train Epoch: [8 / 20]\n",
            "[train-8] step: [200 / 542] | exs = 7995 | loss = 0.0379 | f1 = 100.00 | em = 100.00\n",
            "used_time: 58.50s\n",
            "[train-8] step: [400 / 542] | exs = 8395 | loss = 0.2344 | f1 = 33.33 | em = 50.00\n",
            "used_time: 116.90s\n",
            "Training Epoch 8 -- Loss: 0.0654, F1: 78.14, EM: 77.42 --\n",
            "\n",
            ">>> Dev Epoch: [8 / 20]\n",
            "[predict-8] step: [200 / 531] | f1 = 50.00 | em = 50.00\n",
            "used_time: 17.07s\n",
            "[predict-8] step: [400 / 531] | f1 = 5.88 | em = 0.00\n",
            "used_time: 34.11s\n",
            "Validation Epoch 8 -- F1: 18.11, EM: 17.04 --\n",
            "\n",
            ">>> Train Epoch: [9 / 20]\n",
            "[train-9] step: [200 / 542] | exs = 9080 | loss = 0.0321 | f1 = 56.06 | em = 50.00\n",
            "used_time: 58.39s\n",
            "[train-9] step: [400 / 542] | exs = 9480 | loss = 0.0006 | f1 = 100.00 | em = 100.00\n",
            "used_time: 116.68s\n",
            "Training Epoch 9 -- Loss: 0.0526, F1: 82.36, EM: 82.58 --\n",
            "\n",
            ">>> Dev Epoch: [9 / 20]\n",
            "[predict-9] step: [200 / 531] | f1 = 50.00 | em = 50.00\n",
            "used_time: 17.02s\n",
            "[predict-9] step: [400 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 33.78s\n",
            "Validation Epoch 9 -- F1: 17.83, EM: 16.20 --\n",
            "\n",
            ">>> Train Epoch: [10 / 20]\n",
            "[train-10] step: [200 / 542] | exs = 10165 | loss = 0.0167 | f1 = 100.00 | em = 100.00\n",
            "used_time: 58.19s\n",
            "[train-10] step: [400 / 542] | exs = 10565 | loss = 0.0193 | f1 = 100.00 | em = 100.00\n",
            "used_time: 116.13s\n",
            "Training Epoch 10 -- Loss: 0.0414, F1: 85.80, EM: 87.00 --\n",
            "\n",
            ">>> Dev Epoch: [10 / 20]\n",
            "[predict-10] step: [200 / 531] | f1 = 0.88 | em = 0.00\n",
            "used_time: 17.00s\n",
            "[predict-10] step: [400 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 33.72s\n",
            "Validation Epoch 10 -- F1: 18.45, EM: 17.42 --\n",
            "\n",
            ">>> Train Epoch: [11 / 20]\n",
            "[train-11] step: [200 / 542] | exs = 11250 | loss = 0.0003 | f1 = 50.00 | em = 100.00\n",
            "used_time: 57.99s\n",
            "[train-11] step: [400 / 542] | exs = 11650 | loss = 0.0157 | f1 = 100.00 | em = 100.00\n",
            "used_time: 116.05s\n",
            "Training Epoch 11 -- Loss: 0.0406, F1: 85.38, EM: 87.28 --\n",
            "\n",
            ">>> Dev Epoch: [11 / 20]\n",
            "[predict-11] step: [200 / 531] | f1 = 56.38 | em = 50.00\n",
            "used_time: 16.99s\n",
            "[predict-11] step: [400 / 531] | f1 = 1.56 | em = 0.00\n",
            "used_time: 33.79s\n",
            "Validation Epoch 11 -- F1: 18.21, EM: 17.42 --\n",
            "\n",
            ">>> Train Epoch: [12 / 20]\n",
            "[train-12] step: [200 / 542] | exs = 12335 | loss = 0.0005 | f1 = 100.00 | em = 100.00\n",
            "used_time: 58.47s\n",
            "[train-12] step: [400 / 542] | exs = 12735 | loss = 0.0000 | f1 = 100.00 | em = 100.00\n",
            "used_time: 116.56s\n",
            "Training Epoch 12 -- Loss: 0.0320, F1: 87.07, EM: 89.77 --\n",
            "\n",
            ">>> Dev Epoch: [12 / 20]\n",
            "[predict-12] step: [200 / 531] | f1 = 51.79 | em = 50.00\n",
            "used_time: 17.08s\n",
            "[predict-12] step: [400 / 531] | f1 = 0.00 | em = 0.00\n",
            "used_time: 34.11s\n",
            "Validation Epoch 12 -- F1: 17.56, EM: 15.91 --\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyUriNIJG3KG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}